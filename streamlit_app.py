import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
import os

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Streamlitã®è¨­å®š
st.title("ğŸ¤– AI ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write("ChatGPTã¨Geminiã‚’åˆ‡ã‚Šæ›¿ãˆã¦ä½¿ç”¨ã§ãã¾ã™")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
model_option = st.selectbox(
    "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    ("ChatGPT", "Gemini")
)

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def get_llm():
    if model_option == "ChatGPT":
        return ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            streaming=True
        )
    else:
        return ChatGoogleGenerativeAI(
            model="gemini-pro",
            temperature=0.7,
            convert_system_message_to_human=True
        )

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AIã®å¿œç­”
    with st.chat_message("assistant"):
        llm = get_llm()
        message_placeholder = st.empty()
        full_response = ""

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã®å‡¦ç†
        for chunk in llm.stream(prompt):
            full_response += chunk.content
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    
    # å¿œç­”ã®ä¿å­˜
    st.session_state.messages.append({"role": "assistant", "content": full_response})
